{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Huggingface Tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Group 6 Question 6 - Huggingface BertWordPieceTokenizer\n",
        "Clarissa Cheam, Kavya Jaganathan and Ayushi Mishra"
      ],
      "metadata": {
        "id": "hNvb8WlQvG1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers\n",
        "!pip install transformers\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "import os\n",
        "from transformers import BertTokenizer\n",
        "import math\n",
        "from collections import Counter\n",
        "from tabulate import tabulate\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnFLJ7fJVCBX",
        "outputId": "d03062ad-71cc-4c4a-96c3-5cb9d5d23f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 4.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.11.4\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 32.2 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 51.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.11.4\n",
            "    Uninstalling tokenizers-0.11.4:\n",
            "      Successfully uninstalled tokenizers-0.11.4\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Bert Model**"
      ],
      "metadata": {
        "id": "iowwMnv7Uo_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "UPLOAD SOURCE TEXT AT ROOT"
      ],
      "metadata": {
        "id": "qxs7fAvIVJss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paths = [\"/content/source_text.txt\"]\n",
        "\n",
        "tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text=True,\n",
        "    handle_chinese_chars=False,\n",
        "    strip_accents=False,\n",
        "    lowercase=False\n",
        ")\n",
        "\n",
        "tokenizer.train(files=paths, vocab_size=5000, min_frequency=3,\n",
        "                limit_alphabet=1000, wordpieces_prefix=\"##\",\n",
        "                special_tokens=[\n",
        "                  '[PAD]','[UNK]','[CLS]','[SEP]', '[MASK]'\n",
        "                ])\n",
        "\n",
        "os.mkdir('./bert-it')\n",
        "tokenizer.save_model('./bert-it','bert-it')\n"
      ],
      "metadata": {
        "id": "NEbW42B9XRz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d147f044-c94a-405a-f1a7-1a135e864848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./bert-it/bert-it-vocab.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading saved model"
      ],
      "metadata": {
        "id": "9nIVozEPVUhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('/content/bert-it/bert-it-vocab.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcbYu0mmSat7",
        "outputId": "f668b86c-77ae-46c2-8dc8-e4857017397f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1648: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize Source Text"
      ],
      "metadata": {
        "id": "y8KBRvZ4Vbzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/source_text.txt', 'r') as file:\n",
        "            lines = file.read().replace('\\n', '')\n",
        "corpus = tokenizer(lines)"
      ],
      "metadata": {
        "id": "GX10Wpeprr1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data into train test and val"
      ],
      "metadata": {
        "id": "3YZ1y5NsVles"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_len = math.floor(0.80*len(corpus['input_ids']))\n",
        "val_len = math.ceil(0.10*len(corpus['input_ids']))\n",
        "\n",
        "corpus_train = dict()\n",
        "corpus_val = dict()\n",
        "corpus_test = dict()\n",
        "\n",
        "ii = corpus['input_ids']\n",
        "tti = corpus['token_type_ids']\n",
        "am = corpus['attention_mask']\n",
        "\n",
        "corpus_train['input_ids'] = ii[0:train_len]\n",
        "corpus_train['token_type_ids'] = tti[0:train_len]\n",
        "corpus_train['attention_mask'] = am[0:train_len]\n",
        "\n",
        "corpus_val['input_ids'] = ii[train_len:train_len + val_len]\n",
        "corpus_val['token_type_ids'] = tti[train_len:train_len + val_len]\n",
        "corpus_val['attention_mask'] = am[train_len:train_len + val_len]\n",
        "\n",
        "corpus_test['input_ids'] = ii[train_len + val_len:]\n",
        "corpus_test['token_type_ids'] = tti[train_len + val_len:]\n",
        "corpus_test['attention_mask'] = am[train_len + val_len:]\n"
      ],
      "metadata": {
        "id": "GegL-54zuGGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Term Frequency Dictionary"
      ],
      "metadata": {
        "id": "k7villIiVzWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/bert-it/bert-it-vocab.txt', 'r') as fp:\n",
        "    vocab = fp.read().split('\\n')"
      ],
      "metadata": {
        "id": "0uHnQSz-7YrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_vocabulary = []\n",
        "for i in corpus_train['input_ids']:\n",
        "  train_vocabulary.append(vocab[i])\n",
        "\n",
        "val_vocabulary = []\n",
        "for i in corpus_val['input_ids']:\n",
        "  val_vocabulary.append(vocab[i])\n",
        "\n",
        "test_vocabulary = []\n",
        "for i in corpus_test['input_ids']:\n",
        "  test_vocabulary.append(vocab[i])"
      ],
      "metadata": {
        "id": "ocoYRjt8yZt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_count = dict()\n",
        "val_count = dict()\n",
        "test_count = dict()"
      ],
      "metadata": {
        "id": "m-DgpyEc3l4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in train_vocabulary:\n",
        "    if token in train_count:\n",
        "        train_count[token] = train_count[token] + 1\n",
        "    else:\n",
        "        train_count[token] = 1\n",
        "\n",
        "for token in val_vocabulary:\n",
        "    if token in val_count:\n",
        "        val_count[token] = val_count[token] + 1\n",
        "    else:\n",
        "        val_count[token] = 1\n",
        "\n",
        "for token in test_vocabulary:\n",
        "    if token in test_count:\n",
        "        test_count[token] = test_count[token] + 1\n",
        "    else:\n",
        "        test_count[token] = 1"
      ],
      "metadata": {
        "id": "ITVPBYVb8QqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary Statistics"
      ],
      "metadata": {
        "id": "Qbtd62EyWAKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summary_statistics(frequency_dict, vocabulary, train=False):\n",
        "  to_remove = {}\n",
        "  original_size = len(frequency_dict.keys())\n",
        "  no_of_out_of_vocab = 5000 - original_size\n",
        "  #for key, value in frequency_dict.items():\n",
        "    #if value < 3:\n",
        "        #tot_no_of_unk += value\n",
        "        #no_of_out_of_vocab += 1\n",
        "        #to_remove[key] = value\n",
        "        #print(key, value)\n",
        "  #for k,v in to_remove.items():\n",
        "    #frequency_dict.pop(k)\n",
        "    #frequency_dict['[UNK]'] += v\n",
        "    \n",
        "  if '[UNK]' not in frequency_dict.keys():\n",
        "    tot_no_of_unk = 0\n",
        "  else:\n",
        "    tot_no_of_unk = frequency_dict['[UNK]']\n",
        "\n",
        "  if not train:\n",
        "    no_of_types_unk = 0\n",
        "    for token in vocabulary:\n",
        "        if token not in frequency_dict.keys():\n",
        "            no_of_types_unk += 1\n",
        "\n",
        "  tokens_tag = pos_tag(vocabulary)\n",
        "  counts = Counter(tag for word, tag in tokens_tag)\n",
        "\n",
        "  text = ' '.join(vocabulary)\n",
        "  sentences = text.split(\".\")\n",
        "  words = text.split(\" \")\n",
        "  if (sentences[len(sentences) - 1] == \"\"):\n",
        "      avg = len(words) / len(sentences) - 1\n",
        "  else:\n",
        "      avg = len(words) / len(sentences)\n",
        "\n",
        "  no_of_stop_words = 0\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  for words in vocabulary:\n",
        "      if words in stop_words:\n",
        "          no_of_stop_words += 1\n",
        "\n",
        "  data = []\n",
        "  data.append(len(vocabulary))\n",
        "  # Vocabulary Size\n",
        "  data.append(original_size)\n",
        "  # Num of UNK tokens\n",
        "  data.append(tot_no_of_unk)\n",
        "  # Number of OOV\n",
        "  data.append(no_of_out_of_vocab)\n",
        "  # Number of types mapped to UNK\n",
        "  if train:\n",
        "      data.append('x')\n",
        "  else:\n",
        "      data.append(no_of_types_unk)\n",
        "  # Number of Stop Words\n",
        "  data.append(no_of_stop_words)\n",
        "  # Avg sentence length\n",
        "  data.append(avg)\n",
        "  # POS Tagging\n",
        "  data.append(counts)\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "xFNhkDHP90QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_train = summary_statistics(train_count, train_vocabulary, train=True)"
      ],
      "metadata": {
        "id": "Nq5j-wAVDps1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_val = summary_statistics(val_count, val_vocabulary, train=False)"
      ],
      "metadata": {
        "id": "MbuGq43YD15-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_test = summary_statistics(test_count, test_vocabulary, train=False)"
      ],
      "metadata": {
        "id": "Ss1lohNYL13f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_stats(d_train, d_val, d_test):\n",
        "  table = [['data', 'i', 'ii', 'iii', 'iv', 'v', 'vi', 'Custom Metric 1- Average Sentence Length'],\n",
        "            ['Train'] + d_train[0:7], ['Validation'] + d_val[0:7],\n",
        "            ['Test'] + d_test[0:7]]\n",
        "  print(tabulate(table, headers='firstrow', tablefmt='grid'))\n",
        "  keys_train = []\n",
        "  val_train = []\n",
        "  keys_val = []\n",
        "  val_val = []\n",
        "  keys_test = []\n",
        "  val_test = []\n",
        "  for k, v in d_train[7].items():\n",
        "      keys_train.append(k)\n",
        "      val_train.append(v)\n",
        "  for k, v in d_val[7].items():\n",
        "      keys_val.append(k)\n",
        "      val_val.append(v)\n",
        "  for k, v in d_test[7].items():\n",
        "      keys_test.append(k)\n",
        "      val_test.append(v)\n",
        "  table_train = [keys_train[0:10], val_train[0:10]]\n",
        "  table_val = [keys_val[0:10], val_val[0:10]]\n",
        "  table_test = [keys_test[0:10], val_test[0:10]]\n",
        "  print('vii - Train')\n",
        "  print(tabulate(table_train, headers='firstrow', tablefmt='grid'))\n",
        "  print('vii - Validation')\n",
        "  print(tabulate(table_val, headers='firstrow', tablefmt='grid'))\n",
        "  print('vii - Test')\n",
        "  print(tabulate(table_test, headers='firstrow', tablefmt='grid'))"
      ],
      "metadata": {
        "id": "QWIeztyAMWBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_stats(d_train, d_val, d_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NygMDbAQNQ-g",
        "outputId": "b272b08c-0f78-406f-b191-892a6470d086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------+------+-------+------+-----+--------+--------------------------------------------+\n",
            "| data       |      i |   ii |   iii |   iv | v   |     vi |   Custom Metric 1- Average Sentence Length |\n",
            "+============+========+======+=======+======+=====+========+============================================+\n",
            "| Train      | 699822 | 3359 |     6 | 1641 | x   | 164965 |                                    28.3742 |\n",
            "+------------+--------+------+-------+------+-----+--------+--------------------------------------------+\n",
            "| Validation |  87478 | 2905 |     0 | 2095 | 0   |  20933 |                                    31.3991 |\n",
            "+------------+--------+------+-------+------+-----+--------+--------------------------------------------+\n",
            "| Test       |  87478 | 2936 |     0 | 2064 | 0   |  21622 |                                    32.0902 |\n",
            "+------------+--------+------+-------+------+-----+--------+--------------------------------------------+\n",
            "vii - Train\n",
            "+-------+--------+--------+-------+------+------+-------+------+-------+-------+\n",
            "|    JJ |    NNP |     NN |    IN |    ( |    : |     , |   FW |     . |    CD |\n",
            "+=======+========+========+=======+======+======+=======+======+=======+=======+\n",
            "| 70535 | 137713 | 156250 | 56813 | 5200 | 8157 | 23449 | 4262 | 24811 | 22345 |\n",
            "+-------+--------+--------+-------+------+------+-------+------+-------+-------+\n",
            "vii - Validation\n",
            "+------+-------+------+-------+------+-------+------+------+------+------+\n",
            "|   DT |    NN |   JJ |   NNP |    . |   NNS |   MD |   VB |   IN |   CC |\n",
            "+======+=======+======+=======+======+=======+======+======+======+======+\n",
            "| 4748 | 18764 | 8612 | 18300 | 2802 |  2061 |  461 | 2109 | 7056 | 1512 |\n",
            "+------+-------+------+-------+------+-------+------+------+------+------+\n",
            "vii - Test\n",
            "+-------+-------+------+------+------+-------+------+------+------+------+\n",
            "|    NN |   VBD |   RB |   IN |   JJ |   NNP |   CD |    . |   DT |    , |\n",
            "+=======+=======+======+======+======+=======+======+======+======+======+\n",
            "| 19605 |  2983 | 1486 | 7377 | 9230 | 16613 | 2675 | 2748 | 4897 | 2773 |\n",
            "+-------+-------+------+------+------+-------+------+------+------+------+\n"
          ]
        }
      ]
    }
  ]
}